{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNCCljOPsjQwpS5XK/6O/Oj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ugurklc/Deep_Learning/blob/Master/Natural_Language_Processing/IMDB_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Zkk15DDFcuH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset\n",
        "\n",
        "imdb_sentences = []\n",
        "\n",
        "train_data = tfds.as_numpy(tfds.load(\"imdb_reviews\", split='train'))\n",
        "\n",
        "for item in train_data:\n",
        "  imdb_sentences.append(str(item['text']))"
      ],
      "metadata": {
        "id": "V0e5HN7vNPDA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "tpcL9qJ7E3og",
        "outputId": "0b0ac599-34e2-4fa1-dd96-6a96d1cd273b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'b\"This was an absolutely terrible movie. Don\\'t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\\'s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor\\'s like Christopher Walken\\'s good name. I could barely sit through it.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer and create a set of sequences\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(imdb_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(imdb_sentences)"
      ],
      "metadata": {
        "id": "yN_bUD2BFNch"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(tokenizer.word_index.items())[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXQEG2I0FTwT",
        "outputId": "88eda48a-5d7c-4f32-ff20-bf7ac2e44a49"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 1), ('and', 2), ('a', 3), ('of', 4), ('to', 5), ('is', 6), ('br', 7), ('in', 8), ('it', 9), ('i', 10), ('this', 11), ('that', 12), ('was', 13), ('as', 14), ('for', 15), ('with', 16), ('movie', 17), ('but', 18), ('film', 19), (\"'s\", 20)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove HTML tags\n",
        "# 2. Remove stopwords\n",
        "# 3. Strip punctuation\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "\n",
        "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
        "             \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n",
        "             \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\",\n",
        "             \"he\", \"hed\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
        "             \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\",\n",
        "             \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\",\n",
        "             \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\",\n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\",\n",
        "             \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"this\", \"those\", \"through\",\n",
        "             \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\",\n",
        "             \"what\", \"whats\", \"when\", \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\",\n",
        "             \"whys\", \"with\", \"would\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\",\n",
        "             \"yourselves\"]\n",
        "\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "imdb_sentences = []\n",
        "\n",
        "for item in train_data:\n",
        "    sentence = str(item['text'].decode('UTF-8').lower())\n",
        "    sentence = sentence.replace(\",\", \" , \")\n",
        "    sentence = sentence.replace(\".\", \" . \")\n",
        "    sentence = sentence.replace(\"-\", \" - \")\n",
        "    sentence = sentence.replace(\"/\", \" / \")\n",
        "    soup = BeautifulSoup(sentence)\n",
        "    sentence = soup.get_text()\n",
        "    words = sentence.split()\n",
        "    filtered_sentence = \"\"\n",
        "    for word in words:\n",
        "        word = word.translate(table)\n",
        "        if word not in stopwords:\n",
        "            filtered_sentence = filtered_sentence + word + \" \"\n",
        "    imdb_sentences.append(filtered_sentence)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=25000)\n",
        "tokenizer.fit_on_texts(imdb_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(imdb_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtY8FJXvFsR3",
        "outputId": "8d466d6c-a43b-4a10-9d24-51b7fb601f18"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie', 'film', 'not', 'one', 'like', 'just', 'good', 'no', 'time', 'even', 'story', 'really', 'see', 'can', 'much', 'bad', 'get', 'will', 'also', 'people']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(tokenizer.word_index.items())[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP9HpkpVFs-u",
        "outputId": "4c6a2e66-3970-4142-fd74-d739e1c63055"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('movie', 1), ('film', 2), ('not', 3), ('one', 4), ('like', 5), ('just', 6), ('good', 7), ('no', 8), ('time', 9), ('even', 10), ('story', 11), ('really', 12), ('see', 13), ('can', 14), ('much', 15), ('bad', 16), ('get', 17), ('will', 18), ('also', 19), ('people', 20)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See how the tokenizer worked on simple examples\n",
        "\n",
        "examples = [\n",
        "    'Today is a sunny day',\n",
        "    'Today is a rainy day',\n",
        "    'Is it sunny today?'\n",
        "]"
      ],
      "metadata": {
        "id": "OwSObN_xJ-ZO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sequences = tokenizer.texts_to_sequences(examples)\n",
        "example_sequences"
      ],
      "metadata": {
        "id": "GBs9ZC9dK75F",
        "outputId": "aeb21dfd-08f7-4746-edc4-7444fc7eee57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[516, 5229, 147], [516, 6489, 147], [5229, 516]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See the original sentences but the stopwords removed\n",
        "\n",
        "tokenizer.sequences_to_texts(example_sequences)"
      ],
      "metadata": {
        "id": "Hw_XH2yWLGXw",
        "outputId": "b3ae0cbd-3ad1-4262-b4e6-c6ecc4531cd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['today sunny day', 'today rainy day', 'sunny today']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* See that \"is\", \"a\" and \"it\" are removed from the sentences."
      ],
      "metadata": {
        "id": "Vs41qUSfLTst"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyfDtIIhLRAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}